# PyD2M

PyD2M is a easy management tool of pandas DataFrames.

# Tutorial

In this tutorial, we use PyD2M to manage the data for a simulation of 
*transshipment container port*. In a container port, a t*ransshipment 
container* will be discharged from the first vessel, stored somewhere in 
the terminal's storage yard for a few days. and then be loaded to its
second vessel.

## Loading data
Initially, we have "vessel_info.csv" file generated by some other tool, 
which contains a port's vessel visiting information within one month.
There are three columns in the file: the vessel IDs, vessel lengths and 
their arrival times at the port. Each vessel visits the port in a periodic
pattern; thus, the same VesselID values may appear multiple times in the
file. The ArrivalTime is the number of seconds since the beginning of the
month.

```
VesselId,Length,ArrivalTime
7,240,7148
2,195,9907
0,195,32867
6,216,47490
8,319,49142
3,182,59537
...
```

First, let's creator a new directory named `dataset` and put
`vessel_info.csv` under `dataset/raw'. Also, we creater a sub-directory 
`conf` and create a file `d2m.rc`.

```
dataset
+-- conf
|   +-- d2m.rc
+-- raw
|   +-- vessel_info.csv
```

`{dataset}/conf/d2m.rc` is the default location of a PyD2M dataset's 
configurations. The configuration file is in YAML format. Now, let's add 
the following context to this file:

```
- DATA:
    raw:
      vessel_info.csv:
        TYPE: csv
        DECLARE_NEW_FIELDS: True
        LOCAL_FIELDS_ONLY: False
        FREE_FIELDS: False
        FIELDS:
          - VesselID: str
          - Length: int
          - ArrivalTime: int
```

Then, the data is ready for managing by PyD2M! First, let's import PyD2M 
and create a `DataSource` object:
```
>>> from pyd2m.datasource import DataSource
>>> ds = DataSource("./dataset")
```

Then, we can use the `DataSource.load` method to load the file as a 
pandas DataFrame.
```
>>> ds.load("raw/vessel_info.csv")
	VesselID 	Length 	ArrivalTime
0 	4 	344 	7257
1 	6 	294 	16757
2 	8 	339 	18753
3 	7 	298 	31737
4 	0 	318 	44082
5 	9 	270 	49759
...
```
Or, we can directly use the field names to load the data:
```
>>> ds["VesselID", "Length", "ArrivalTime"]
Base:  raw/vessel_info.csv

	VesselID 	Length 	ArrivalTime
0 	4 	344 	7257
1 	6 	294 	16757
2 	8 	339 	18753
3 	7 	298 	31737
4 	0 	318 	44082
5 	9 	270 	49759
...
```
Also, we can access only partial of the data and change the order of columns:
```
>> ds["ArrivalTime", "Length"]
Base:  raw/vessel_info.csv

 	ArrivalTime 	Length
0 	7257 	344
1 	16757 	294
2 	18753 	339
3 	31737 	298
4 	44082 	318
5 	49759 	270
...
```

However, one may find the original data is not convenient for data analytics.
For example, we would like the `ArrivalTime` to be in a more human-readable 
`datatime` format instead of the integers. And also, we would like to add
a unique `VesselArrivalID` to each visit of the vessels, so that later we can 
refer to the visit more easily. The `VesselArrivalID` will be defined as 
`{MMDD}V{VesselID}`.

Thus, let's add a **hook** to this file. First, we create a 
**hook file** named `vessels.hk` and put it into the `conf` sub-directory.
<pre><code>
dataset
+-- conf
|   +-- d2m.rc
|   +-- <b>vessels.hk</b>
+-- raw
|   +-- vessel_info.csv
</pre>

* **Note**: The names of the hook file can be arbitrary, as long as their
extension names are `.hk`. there can be multiple `.hk` files in the `conf`
directory, and they will be loaded automatically.

Then, we add the following context to the `vessels.hk` file.
```
from pyd2m import hooks
import pandas as pd


@hooks.load("raw/vessel_info.csv")
def vel_load_hook(df):
    df.ArrivalTime = pd.to_timedelta(df.ArrivalTime, unit="s") + pd.to_datetime("2019")
    df["VesselArrivalID"] = df.ArrivalTime.dt.strftime("%m%d") + "V" + df.VesselID.astype(str)
    return df
```

Also, we need to change `ArrivalTime`'s type to `datatime64[s]` and add
the `VesselArrivalID`.
```
- DATA:
    raw:
      vessel_info.csv:
        TYPE: csv
        DECLARE_NEW_FIELDS: True
        LOCAL_FIELDS_ONLY: False
        FREE_FIELDS: False
        FIELDS:
          - VesselID: str
          - Length: int
          - ArrivalTime: datetime64[s]
          - VesselArrivalID: str
```

Let's load this file/fields again:
```
>>> ds = DataSource("./dataset")
>>> ds["VesselArrivalID", "ArrivalTime", "Length"]
Base:  raw/vessel_info.csv

	VesselArrivalID 	ArrivalTime 	Length
0 	0101V7 	2019-01-01 04:13:07 	101
1 	0101V0 	2019-01-01 06:03:26 	335
2 	0101V5 	2019-01-01 06:27:19 	150
3 	0101V3 	2019-01-01 12:34:50 	314
4 	0101V2 	2019-01-01 14:19:19 	396
5 	0101V8 	2019-01-01 14:38:38 	140
```

You may have noticed that, in `vel_load_hook` function, the `ArrivalTime` columns 
of the transformed DataFrame actually has the type of `datetime64[ns]`. However,
since we have declare the `ArrivalTime` to be `datetime64[s]` in the configuration.
Its type has already been converted when doing loading!

Now, suppose we have another csv file `box_info.csv` records the containers'
information, including their `BoxID`, `UnloadingVesselArrivalID` and 
`LoadingVesselArrivalID`. Let's add this file's information in the configuration
file as well.
```
- DATA:
    raw:
      vessel_info.csv:
        ...
        
      box_info.csv:
        TYPE: csv
        DECLARE_NEW_FIELDS: True
        LOCAL_FIELDS_ONLY: False
        FREE_FIELDS: False
        FIELDS:
          - BoxID: str
          - UnloadingVesselArrivalID: str
          - LoadingVesselArrivalID: str
```

We can also define a `DEFAULT` section in the configuration file and move 
the common attributes of different files into it, so that we don't need to
declare it every time.
```
- DEFAULTS:
    TYPE: csv
    DECLARE_NEW_FIELDS: True
    LOCAL_FIELDS_ONLY: False
    FREE_FIELDS: False

- DATA:
    raw:
      vessel_info.csv:
        FIELDS:
          - VesselID: str
          - Length: int
          - ArrivalTime: datetime64[s]
          - VesselArrivalID: str

      box_info.csv:
        FIELDS:
          - BoxID: str
          - UnloadingVesselArrivalID: str
          - LoadingVesselArrivalID: str
```

## Saving data

When a vessel arrives at the container port, the *berth planner* will decide
when and where the vessel can be berthed. Here, we will not dive into the
complicated vessel berthing algorithms. Let's write a simple random
function to berth the vessels --- making each vessel wait for random time within
2 hours after its arrival, and then put it on a random position along the linear quay of the port.
After being berthed, each vessel will have a random handling time between 
4 and 12 hours.

First, let's define some const values in the dataset's configuration file.
```
- PARAMS:
    QUAY_LENGTH: 3000
    MAX_WAITING_TIME: 7200 
```

Then, these consts can be directly accessed via `ds.{var_name}`. 

```
>>> df = ds["VesselArrivalID", "Length", "ArrivalTime"]
>>> df["MooringPosition"] = df.apply(lambda v: np.random.randint(0, ds.QUAY_LENGTH - v.Length), axis=1)
>>> df["MooringTime"] = df.ArrivalTime + pd.to_timedelta(np.random.random(size=len(df)) * ds.MAX_WAITING_TIME, unit="s")
>>> df["HandlingTime"] = pd.to_timedelta(np.random.uniform(4, 12, size=len(df)), unit="h")
>>> df["HandlingTime"] = pd.to_timedelta(np.random.uniform(4, 12, size=len(df)), unit="h")
```

After that, we can save the data into another file. First, we declare a file
in the configuration file. This time, we want the file to be in *msgpack*
format.
```
- DATA:
    raw:
      ... 
      
    plan:
      berthing.msg:
        TYPE: msgpack
        FIELDS:
          - VesselArrivalID: str
          - MooringPosition: int
          - MooringTime: datetime64[s]
          - HandlingTime: timedelta64[s]
```

Then, use the `DataSource.dump` to save the data.
```
>> ds.dump("plan/berthing.msg", df)
```

* **Tips**: a hook can also be added to the dumping process. Just use the 
`@hooks.dump` to decorate a function in any `.hk` files.

Once the file exists, we can use `load` to load it again, or access its
field directly.
```
>>> ds.load("plan/berthing.msg", df)
 	VesselArrivalID 	MooringPosition 	MooringTime 	HandlingTime
0 	0101V8 	816 	2019-01-01 02:25:22 	04:42:51
1 	0101V7 	243 	2019-01-01 05:57:56 	07:32:13
2 	0101V0 	630 	2019-01-01 06:12:13 	07:12:19
3 	0101V9 	140 	2019-01-01 07:51:36 	11:30:00
4 	0101V6 	570 	2019-01-01 09:29:30 	07:43:05
5 	0101V3 	588 	2019-01-01 13:05:37 	08:27:04
...
>>> ds["VesselArrivalID", "MooringTime"]
Base:  plan/berthing.msg

	VesselArrivalID 	MooringTime
0 	0101V8 	2019-01-01 02:25:22
1 	0101V7 	2019-01-01 05:57:56
2 	0101V0 	2019-01-01 06:12:13
3 	0101V9 	2019-01-01 07:51:36
4 	0101V6 	2019-01-01 09:29:30
5 	0101V3 	2019-01-01 13:05:37
...
```

## Auto joining

Now, what if we want to analyse the relationships between vessels' lengths 
and their handling times? To do so, we need the values of both `Length` 
and `HandlingTime`.  However, there are in different files and of different 
format! Do we need to load these two files seperately and join them manually?
The answer is of course not. Instead, we can retrieve these fields directly.
```
>>> ds["VesselArrivalID", "Length", "HandlingTime"]
Base:  raw/vessel_info.csv
Joining: plan/berthing.msg

	VesselArrivalID 	Length 	HandlingTime
0 	0101V8 	345 	09:12:38
1 	0101V7 	217 	09:47:53
2 	0101V0 	293 	08:55:18
3 	0101V9 	177 	05:42:23
4 	0101V6 	115 	10:06:20
5 	0101V3 	375 	10:11:21
...
```
See? PyD2M has done this joining automatically! 

## Cookbook

A *cookbook* contains a series of *recipes*. Each recipe is a function which
generate new dataframes (*dishes*) using exists dataframes (ingredients).

Now, let's generate each container's unloading/loading time and position 
at the quay according to the vessel information. The results will be saved
in `plan/box_pos_time.msg`. First, add the file's information in the
configuration file.
```
- DATA:
    ...
    
    plan:
      ...
      
      box_pos_time.msg:
        TYPE: msgpack
        FIELDS:
          - BoxID: str
          - UnloadingPosition: int
          - UnloadingTime: datetime64[s]
          - LoadingPosition: int
          - LoadingTime: datetime64[s]
```

Then, create a file `plan.cb` in `conf` directory and add the following
code to it. Again, the filename can be arbitrary as long as the extension 
name is `.cb`. 
```
@recipe("plan/box_pos_time.msg")
def gen_box_pos_time(cb):
    vel_info = cb.DS["VesselArrivalID", "MooringPosition", "Length", "MooringTime", "HandlingTime"]

    df_u = cb.DS["BoxID", "UnloadingVesselArrivalID"].merge(
        vel_info, left_on="UnloadingVesselArrivalID", right_on="VesselArrivalID")

    df_u["UnloadingPosition"] = df_u.Length * np.random.random(size=len(df_u)) + df_u.MooringPosition
    df_u["UnloadingTime"] = df_u.HandlingTime * np.random.random(size=len(df_u)) + df_u.MooringTime

    df_l = cb.DS["BoxID", "LoadingVesselArrivalID"].merge(
        vel_info, left_on="LoadingVesselArrivalID", right_on="VesselArrivalID")

    df_l["LoadingPosition"] = df_l.Length * np.random.random(size=len(df_l)) + df_l.MooringPosition
    df_l["LoadingTime"] = df_l.HandlingTime * np.random.random(size=len(df_l)) + df_l.MooringTime

    return df_u.merge(df_l, on="BoxID")
```

* **Note**: We can also indicate the recipe's ingredients and dishes 
manually as follows.
```
@recipe(ingredients=["raw/vessel_info.csv", "plan/berthing.msg"], dishes=["plan/box_pos_time.msg"])
def gen_box_pos_time(cb, vel, bth):
    vel_info = vel.merge(bth, on="VesselArrivalID")
    ...
```
In this case, the parameters after `cb` are the DataFrames in the 
ingredient list, separately.

Now, let load boxes' unloading information directly.
```
>>>ds["BoxID", "UnloadingVesselArrivalID", "UnloadingPosition", "UnloadingTime]
Generating plan/box_pos_time.msg
[] => ['plan/box_pos_time.msg'] By <CookBook.gen_box_pos_time>
Base:  plan/berthing.msg
Joining: raw/vessel_info.csv
Base:  raw/box_info.csv
Base:  raw/box_info.csv
Base:  plan/box_pos_time.msg
Joining: raw/box_info.csv

	BoxID 	UnloadingVesselArrivalID 	UnloadingPosition 	UnloadingTime
0 	0 	0101V0 	2510 	2019-01-01 13:20:21
1 	105 	0101V0 	2737 	2019-01-01 13:54:03
2 	142 	0101V0 	2544 	2019-01-01 08:08:59
3 	224 	0101V0 	2388 	2019-01-01 14:47:23
4 	283 	0101V0 	2727 	2019-01-01 11:25:09
5 	324 	0101V0 	2707 	2019-01-01 07:26:47
```
The `box_pos_time.msg` has been generated automatically and the fields are
extracted/joined correctly! Now, the structure of the `dataset` directory
is as follows.
<pre><code>
dataset
+-- conf
|   +-- d2m.rc
|   +-- vessels.hk
|   +-- <b>plan.cb</b>
+-- raw
|   +-- vessel_info.csv
|   +-- box_info.csv
+-- plan
|   +-- berthing.msg
|   +-- <b>box_pos_time.msg</b>
</pre>


We can also add the recipe of file `berthing.msg` into the cookbook, by
adding the following to `plan.cb` (or another `.cb` file, there can be as
many as you want `.cb` files in the `conf` directory and they will all be
loaded automatically). 
```
@recipe("plan/berthing.msg")
def gen_berthing_plan(cb):
    df = cb.DS["VesselArrivalID", "Length", "ArrivalTime"]
    df["MooringPosition"] = df.apply(lambda v: np.random.randint(0, cb.DS.QUAY_LENGTH - v.Length), axis=1)
    df["MooringTime"] = df.ArrivalTime + pd.to_timedelta(np.random.random(size=len(df)) * cb.DS.MAX_WAITING_TIME, unit="s")
    df["HandlingTime"] = pd.to_timedelta(np.random.uniform(4, 12, size=len(df)), unit="h")
    df["HandlingTime"] = pd.to_timedelta(np.random.uniform(4, 12, size=len(df)), unit="h")
    return df
```

Now, let's delete the whole `plan` directory under `dataset`, and then access
boxes' information.
```
>>> ds["BoxID", "LoadingVesselArrivalID", "LoadingPosition", "LoadingTime"]
Generating plan/box_pos_time.msg
[] => ['plan/box_pos_time.msg'] By <CookBook.gen_box_pos_time>
Generating plan/berthing.msg
['raw/vessel_info.csv'] => ['plan/berthing.msg'] By <CookBook.gen_berthing_plan>
Base:  plan/berthing.msg
Joining: raw/vessel_info.csv
Base:  raw/box_info.csv
Base:  raw/box_info.csv
Base:  plan/box_pos_time.msg
Joining: raw/box_info.csv

	BoxID 	LoadingVesselArrivalID 	LoadingPosition 	LoadingTime
0 	0 	0106V3 	625 	2019-01-06 11:02:05
1 	84 	0128V4 	2648 	2019-01-28 08:12:10
2 	105 	0110V1 	2002 	2019-01-11 00:01:40
3 	129 	0119V1 	1144 	2019-01-19 23:47:53
4 	132 	0116V6 	2626 	2019-01-16 19:53:44
5 	142 	0101V2 	2406 	2019-01-02 07:28:23
...
```
Whoosh! The data have returned!

# APIs
## DataSource
## Cookbook
## Hooks