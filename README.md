# PyD2M

PyD2M is a easy management tool of pandas DataFrames.

# Tutorial (**unfinished**)

In this tutorial, we use PyD2M to manage the data for a simulation of 
*transshipment container port*. In a container port, a t*ransshipment 
container* will be discharged from the first vessel, stored somewhere in 
the terminal's storage yard for a few days. and then be loaded to its
second vessel.

## Loading data
Initially, we have "vessel_info.csv" file generated by some other tool, 
which contains a port's vessel visiting information within one month.
There are three columns in the file: the vessel IDs, vessel lengths and 
their arrival times at the port. Each vessel visits the port in a periodic
pattern; thus, the same VesselID values may appear multiple times in the
file. The ArrivalTime is the number of seconds since the beginning of the
month.

```
VesselId,Length,ArrivalTime
7,240,7148
2,195,9907
0,195,32867
6,216,47490
8,319,49142
3,182,59537
...
```

First, let's creator a new directory named `dataset` and put
`vessel_info.csv` under `dataset\raw'. Also, we creater a sub-directory 
`conf` and create a file `d2m.rc`.

```
dataset
+-- conf
|   +-- d2m.rc
+-- raw
|   +-- vessel_info.csv
```

`{dataset}\conf\d2m.rc` is the default location of a PyD2M dataset's 
configurations. The configuration file is in YAML format. Now, let's add 
the following context to this file:

```
- DATA:
    raw:
      vessel_info.csv:
        TYPE: csv
        DECLARE_NEW_FIELDS: True
        LOCAL_FIELDS_ONLY: False
        FREE_FIELDS: False
        FIELDS:
          - VesselID: str
          - Length: int
          - ArrivalTime: int
```

Then, the data is ready for managing by PyD2M! First, let's import PyD2M 
and create a `DataSource` object:

```
>>> from pyd2m.datasource import DataSource
>>> ds = DataSource("./dataset")
```

Then, we can use

```
>>> ds.load("raw/vessel_info.csv")
	VesselID 	Length 	ArrivalTime
0 	4 	344 	7257
1 	6 	294 	16757
2 	8 	339 	18753
3 	7 	298 	31737
4 	0 	318 	44082
5 	9 	270 	49759
...
```
to load the file, as a pandas DataFrame. Or, we can directly use the field
names to load the data:
```
>>> ds["VesselID", "Length", "ArrivalTime"]
Base:  raw/vessel_info.csv
	VesselID 	Length 	ArrivalTime
0 	4 	344 	7257
1 	6 	294 	16757
2 	8 	339 	18753
3 	7 	298 	31737
4 	0 	318 	44082
5 	9 	270 	49759
...
```
Also, we can access only partial of the data and change the order of columns:
```
>> ds["ArrivalTime", "Length"]
Base:  raw/vessel_info.csv
 	ArrivalTime 	Length
0 	7257 	344
1 	16757 	294
2 	18753 	339
3 	31737 	298
4 	44082 	318
5 	49759 	270
...
```

However, one may find the original data is not convenient for data analytics.
For example, we would like the `ArrivalTime` to be in a more human-readable 
`datatime` format instead of the integers. And also, we would like to add
a unique `VesselArrivalID` to each visit of the vessels, so that later we can 
refer to the visit more easily. The `VesselArrivalID` will be defined as 
`{MMDD}V{VesselID}`.

To do this, let's add a **hook** to this file. First, we create a 
**hook file** named `vessels.hk` and put it into the `conf` sub-directory.
The filename can be arbitrary, but the extension name must be `.hk`. As long as
a `.hk` file is in the `conf` directory, it will be loaded automatically.
<pre><code>
dataset
+-- conf
|   +-- d2m.rc
|   +-- <b>vessels.hk</b>
+-- raw
|   +-- vessel_info.csv
</pre>

Then, we add the following context to the `vessels.hk` file.
```
from pyd2m import hooks
import pandas as pd


@hooks.load("raw/vessel_info.csv")
def vel_load_hook(df):
    df.ArrivalTime = pd.to_timedelta(df.ArrivalTime, unit="s") + pd.to_datetime("2019")
    df["VesselArrivalID"] = df.ArrivalTime.dt.strftime("%m%d") + "V" + df.VesselID.astype(str)
    return df
```

Also, we need to change `ArrivalTime`'s type to `datatime64[s]` and add
the `VesselArrivalID`.
```
- DATA:
    raw:
      vessel_info.csv:
        TYPE: csv
        DECLARE_NEW_FIELDS: True
        LOCAL_FIELDS_ONLY: False
        FREE_FIELDS: False
        FIELDS:
          - VesselID: str
          - Length: int
          - ArrivalTime: datetime64[s]
          - VesselArrivalID: str
```

Let's load this file/fields again:
```
>>> ds = DataSource("./dataset")
>>> ds["VesselArrivalID", "ArrivalTime", "Length"]
Base:  raw/vessel_info.csv
	VesselArrivalID 	ArrivalTime 	Length
0 	0101V7 	2019-01-01 04:13:07 	101
1 	0101V0 	2019-01-01 06:03:26 	335
2 	0101V5 	2019-01-01 06:27:19 	150
3 	0101V3 	2019-01-01 12:34:50 	314
4 	0101V2 	2019-01-01 14:19:19 	396
5 	0101V8 	2019-01-01 14:38:38 	140
```

You may have noticed that, in `vel_load_hook` function, the `ArrivalTime` columns 
of the transformed DataFrame actually has the type of `datetime64[ns]`. However,
since we have declare the `ArrivalTime` to be `datetime64[s]` in the configuration.
Its type has already been converted when doing loading!

Now, suppose we have another csv file `box_info.csv` records the containers'
information, including their `BoxID`, `UnloadingVesselArrivalID` and 
`LoadingVesselArrivalID`. Let's add this file's information in the configuration
file as well.
```
- DATA:
    raw:
      vessel_info.csv:
        ...
        
      box_info.csv:
        TYPE: csv
        DECLARE_NEW_FIELDS: True
        LOCAL_FIELDS_ONLY: False
        FREE_FIELDS: False
        FIELDS:
          - BoxID: str
          - UnloadingVesselArrivalID: str
          - LoadingVesselArrivalID: str
```

We can also define a `DEFAULT` section in the configuration file and move 
the common attributes of different files into it, so that we don't need to
declare it every time.
```
- DEFAULTS:
    TYPE: csv
    DECLARE_NEW_FIELDS: True
    LOCAL_FIELDS_ONLY: False
    FREE_FIELDS: False

- DATA:
    raw:
      vessel_info.csv:
        FIELDS:
          - VesselID: str
          - Length: int
          - ArrivalTime: datetime64[s]
          - VesselArrivalID: str

      box_info.csv:
        FIELDS:
          - BoxID: str
          - UnloadingVesselArrivalID: str
          - LoadingVesselArrivalID: str
```

## Saving data

When a vessel arrives at the container port, the *berth planner* will decide
when and where the vessel can be berthed. Here, we will not dive into the
complicated vessel berthing algorithms. Let's write a simple random
function to berth the vessels --- making each vessel wait for random time within
2 hours after its arrival, and then put it on a random position along the linear quay of the port.
After being berthed, each vessel will have a random handling time between 
4 and 12 hours.

First, let's define some const values in the dataset's configuration file.
```
- PARAMS:
    QUAY_LENGTH: 3000
    MAX_WAITING_TIME: 7200 
```

Then, these consts can be directly accessed via `ds.{var_name}`. 

```
>>> df = ds["VesselArrivalID", "Length", "ArrivalTime"]
>>> df["MooringPosition"] = df.apply(lambda v: np.random.randint(0, ds.QUAY_LENGTH - v.Length), axis=1)
>>> df["MooringTime"] = df.ArrivalTime + pd.to_timedelta(np.random.random(size=len(df)) * ds.MAX_WAITING_TIME, unit="s")
>>> df["HandlingTime"] = pd.to_timedelta(np.random.uniform(4, 12, size=len(df)), unit="h")
>>> df["HandlingTime"] = pd.to_timedelta(np.random.uniform(4, 12, size=len(df)), unit="h")
```

After that, we can save the data into another file. First, we declare a file
in the configuration file. This time, we want the file to be in *msgpack*
format.
```
- DATA:
    raw:
      ... 
      
    plan:
      berthing.msg:
        TYPE: msgpack
        FIELDS:
          - VesselArrivalID: str
          - MooringPosition: int
          - MooringTime: datetime64[s]
          - HandlingTime: timedelta64[s]
```

Then, use the following code to save the data.
```
>> ds.dump("plan/berthing.msg", df)
```

Once the file exists, we can use `load` to load it again, or access its
field directly.
```
>>> ds.load("plan/berthing.msg", df)
 	VesselArrivalID 	MooringPosition 	MooringTime 	HandlingTime
0 	0101V8 	816 	2019-01-01 02:25:22 	04:42:51
1 	0101V7 	243 	2019-01-01 05:57:56 	07:32:13
2 	0101V0 	630 	2019-01-01 06:12:13 	07:12:19
3 	0101V9 	140 	2019-01-01 07:51:36 	11:30:00
4 	0101V6 	570 	2019-01-01 09:29:30 	07:43:05
5 	0101V3 	588 	2019-01-01 13:05:37 	08:27:04
...
>>> ds["VesselArrivalID", "MooringTime"]
Base:  plan/berthing.msg
	VesselArrivalID 	MooringTime
0 	0101V8 	2019-01-01 02:25:22
1 	0101V7 	2019-01-01 05:57:56
2 	0101V0 	2019-01-01 06:12:13
3 	0101V9 	2019-01-01 07:51:36
4 	0101V6 	2019-01-01 09:29:30
5 	0101V3 	2019-01-01 13:05:37
...
```

## Auto joining


  

# APIs
## DataSource
## Cookbook
## Hooks